{
  "_id": { "$oid": "68627c3ac6ce8cf75ea7563a" },
  "name": "google search batch",
  "description": "Performs searches for a list of queries. Uses Google Custom Search API when credentials exist, otherwise falls back to DuckDuckGo (ddgs). Returns a single JSON object containing a single PDF-ready string when pdf=true, with all queries as headings and combined content.",
  "type": "atomic",
  "execution_mode": "sandboxed",
  "mode": "ALL",
  "platforms": ["linux", "windows", "darwin"],
  "input_schema": {
    "queries": {
      "type": "array",
      "items": { "type": "string" },
      "example": ["latest AI news", "best cloud providers 2025"],
      "description": "List of search queries."
    },
    "num_results": {
      "type": "integer",
      "example": 5,
      "description": "Number of results per query."
    },
    "pdf": {
      "type": "boolean",
      "example": false,
      "description": "If true, returns a single JSON object with all queries as headings and combined content in one string."
    }
  },
  "output_schema": {
    "results": {
      "type": "string",
      "description": "When pdf=true, returns a single string with all queries as headings and their combined content. When pdf=false, returns a JSON list of search results per query."
    }
  },
  "code": "import os, sys, json, asyncio, importlib, subprocess, random, re\nfrom typing import List\n\n# --- Ensure dependencies ---\ndef _ensure(pkg: str):\n    try:\n        importlib.import_module(pkg)\n    except ImportError:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--quiet'])\n\nfor pkg in ['ddgs', 'aiohttp', 'trafilatura', 'google-api-python-client']:\n    _ensure(pkg)\n\nimport trafilatura\nfrom aiohttp import ClientSession, ClientTimeout\nfrom ddgs import DDGS\n\nUA_LIST = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36',\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',\n    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0'\n]\n\ndef _random_ua(): return random.choice(UA_LIST)\n\ndef _normalise_ws(t: str) -> str:\n    return re.sub(r\"\\s+\", \" \", (t or '')).strip()\n\ndef _strip_links_images(t: str) -> str:\n    return re.sub(r'!\\[.*?\\]\\([^)]*\\)', '', t or '')\n\nasync def _fetch(session, url, timeout_sec=10):\n    try:\n        async with session.get(url, timeout=timeout_sec) as r:\n            if r.status == 200:\n                return await r.text()\n    except Exception:\n        return ''\n    return ''\n\nasync def duckduckgo_search(query, num_results, timeout_sec=10):\n    results = []\n    sem = asyncio.Semaphore(5)\n\n    def _pick_mode(q):\n        q = q.lower()\n        if any(w in q for w in ['video', 'youtube', 'trailer']): return 'videos'\n        if any(w in q for w in ['image', 'photo', 'wallpaper']): return 'images'\n        if any(w in q for w in ['news', 'headline']): return 'news'\n        return 'text'\n\n    mode = _pick_mode(query)\n    timeout = ClientTimeout(total=timeout_sec)\n\n    async with ClientSession(timeout=timeout, headers={'User-Agent': _random_ua()}) as session:\n        dd = DDGS()\n        with dd:\n            if mode == 'videos': hits = list(dd.videos(query, max_results=num_results))\n            elif mode == 'images': hits = list(dd.images(query, max_results=num_results))\n            elif mode == 'news': hits = list(dd.news(query, max_results=num_results))\n            else: hits = list(dd.text(query, max_results=num_results))\n\n            async def _process(hit):\n                async with sem:\n                    url = hit.get('url') or hit.get('href')\n                    title = hit.get('title') or 'Untitled'\n                    snippet = hit.get('description') or ''\n\n                    entry = {\n                        'title': _normalise_ws(title),\n                        'url': url,\n                        'content': _normalise_ws(snippet),\n                        'type': mode\n                    }\n\n                    if mode == 'text' and url:\n                        html = await _fetch(session, url)\n                        extracted = trafilatura.extract(html) or entry['content']\n                        entry['content'] = _strip_links_images(_normalise_ws(extracted))\n\n                    results.append(entry)\n\n            await asyncio.gather(*(_process(h) for h in hits))\n\n    return [r for r in results if r.get('url')]\n\nasync def google_search(query, num_results=5):\n    try:\n        from googleapiclient.discovery import build\n        api_key = os.getenv('GOOGLE_API_KEY')\n        cse_id = os.getenv('GOOGLE_CSE_ID')\n        if not api_key or not cse_id:\n            raise Exception('No API key')\n\n        service = build('customsearch', 'v1', developerKey=api_key)\n        res = service.cse().list(q=query, cx=cse_id, num=num_results).execute()\n        items = res.get('items', [])\n\n        return [{\n            'title': _normalise_ws(i.get('title', 'Untitled')),\n            'url': i.get('link'),\n            'content': _normalise_ws(i.get('snippet', '')),\n            'type': 'text'\n        } for i in items]\n\n    except Exception:\n        return await duckduckgo_search(query, num_results)\n\nasync def batch_search(queries, num_results, pdf):\n    all_texts = []\n    for q in queries:\n        results = await google_search(q, num_results)\n        combined_content = ' '.join(\n            re.sub(r'\\s+', ' ', item.get('content') or '').strip()\n            for item in results if item.get('content')\n        )\n        if combined_content:\n            all_texts.append(f\"{q}:\\n{combined_content}\")\n\n    if pdf:\n        # Wrap single string in JSON so agent can parse\n        return json.dumps({\"results\": '\\n\\n'.join(all_texts)})\n    else:\n        out = []\n        for q in queries:\n            results = await google_search(q, num_results)\n            out.append({'query': q, 'search_results': results})\n        return json.dumps(out)\n\n# --- Entrypoint ---\ninput_data = globals().get('input_data', {})\nqueries = input_data.get('queries', [])\nnum_results = int(input_data.get('num_results', 5))\npdf = bool(input_data.get('pdf', False))\n\nif not queries:\n    print(\"'queries' must be a non-empty list\")\n    raise SystemExit\n\nout = asyncio.run(batch_search(queries, num_results, pdf))\nprint(out)",
  "platform_overrides": {
    "windows": {
      "code": "import os, sys, json, asyncio, importlib, subprocess, random, re\nfrom typing import List\n\n# --- Windows event loop fix ---\nif sys.platform.startswith('win'):\n    try:\n        from asyncio import WindowsSelectorEventLoopPolicy\n        asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())\n    except ImportError:\n        pass\n\n# --- Ensure dependencies ---\ndef _ensure(pkg: str):\n    try:\n        importlib.import_module(pkg)\n    except ImportError:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--quiet'])\n\nfor pkg in ['ddgs', 'aiohttp', 'trafilatura', 'google-api-python-client']:\n    _ensure(pkg)\n\nimport trafilatura\nfrom aiohttp import ClientSession, ClientTimeout\nfrom ddgs import DDGS\n\nUA_LIST = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36',\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',\n    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0'\n]\n\ndef _random_ua(): return random.choice(UA_LIST)\n\ndef _normalise_ws(t: str) -> str:\n    return re.sub(r\"\\s+\", \" \", (t or '')).strip()\n\ndef _strip_links_images(t: str) -> str:\n    return re.sub(r'!\\[.*?\\]\\([^)]*\\)', '', t or '')\n\nasync def _fetch(session, url, timeout_sec=10):\n    try:\n        async with session.get(url, timeout=timeout_sec) as r:\n            if r.status == 200:\n                return await r.text()\n    except Exception:\n        return ''\n    return ''\n\nasync def duckduckgo_search(query, num_results, timeout_sec=10):\n    results = []\n    sem = asyncio.Semaphore(5)\n\n    def _pick_mode(q):\n        q = q.lower()\n        if any(w in q for w in ['video', 'youtube', 'trailer']): return 'videos'\n        if any(w in q for w in ['image', 'photo', 'wallpaper']): return 'images'\n        if any(w in q for w in ['news', 'headline']): return 'news'\n        return 'text'\n\n    mode = _pick_mode(query)\n    timeout = ClientTimeout(total=timeout_sec)\n\n    async with ClientSession(timeout=timeout, headers={'User-Agent': _random_ua()}) as session:\n        dd = DDGS()\n        with dd:\n            if mode == 'videos': hits = list(dd.videos(query, max_results=num_results))\n            elif mode == 'images': hits = list(dd.images(query, max_results=num_results))\n            elif mode == 'news': hits = list(dd.news(query, max_results=num_results))\n            else: hits = list(dd.text(query, max_results=num_results))\n\n            async def _process(hit):\n                async with sem:\n                    url = hit.get('url') or hit.get('href')\n                    title = hit.get('title') or 'Untitled'\n                    snippet = hit.get('description') or ''\n\n                    entry = {\n                        'title': _normalise_ws(title),\n                        'url': url,\n                        'content': _normalise_ws(snippet),\n                        'type': mode\n                    }\n\n                    if mode == 'text' and url:\n                        html = await _fetch(session, url)\n                        extracted = trafilatura.extract(html) or entry['content']\n                        entry['content'] = _strip_links_images(_normalise_ws(extracted))\n\n                    results.append(entry)\n\n            await asyncio.gather(*(_process(h) for h in hits))\n\n    return [r for r in results if r.get('url')]\n\nasync def google_search(query, num_results=5):\n    try:\n        from googleapiclient.discovery import build\n        api_key = os.getenv('GOOGLE_API_KEY')\n        cse_id = os.getenv('GOOGLE_CSE_ID')\n        if not api_key or not cse_id:\n            raise Exception('No API key')\n\n        service = build('customsearch', 'v1', developerKey=api_key)\n        res = service.cse().list(q=query, cx=cse_id, num=num_results).execute()\n        items = res.get('items', [])\n\n        return [{\n            'title': _normalise_ws(i.get('title', 'Untitled')),\n            'url': i.get('link'),\n            'content': _normalise_ws(i.get('snippet', '')),\n            'type': 'text'\n        } for i in items]\n\n    except Exception:\n        return await duckduckgo_search(query, num_results)\n\nasync def batch_search(queries, num_results, pdf):\n    all_texts = []\n    for q in queries:\n        results = await google_search(q, num_results)\n        combined_content = ' '.join(\n            re.sub(r'\\s+', ' ', item.get('content') or '').strip()\n            for item in results if item.get('content')\n        )\n        if combined_content:\n            all_texts.append(f\"{q}:\\n{combined_content}\")\n\n    if pdf:\n        # Wrap single string in JSON so agent can parse\n        return json.dumps({\"results\": '\\n\\n'.join(all_texts)})\n    else:\n        out = []\n        for q in queries:\n            results = await google_search(q, num_results)\n            out.append({'query': q, 'search_results': results})\n        return json.dumps({\"results\": out})\n\n# --- Entrypoint ---\ninput_data = globals().get('input_data', {})\nqueries = input_data.get('queries', [])\nnum_results = int(input_data.get('num_results', 5))\npdf = bool(input_data.get('pdf', False))\n\nif not queries:\n    print(\"'queries' must be a non-empty list\")\n    raise Exception('Error executing Google search batch')\n\nout = asyncio.run(batch_search(queries, num_results, pdf))\nprint(out)"
    },
    "darwin": {
      "code": "import ssl, certifi\nssl_context = ssl.create_default_context(cafile=certifi.where())"
    }
  },
  "default": false,
  "scope": ["global"]
}